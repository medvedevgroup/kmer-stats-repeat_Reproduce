Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 100
Rules claiming more threads will be scaled down.
Job stats:
job                       count
----------------------  -------
all                           1
plot_hamming_distance         1
plot_kmer_distribution        1
run_kmer_info                 1
total                         4

Select jobs to execute...
Execute 1 jobs...

[Mon May  5 20:08:08 2025]
localrule run_kmer_info:
    input: ../sequence_data/chr6.fasta
    output: chr6_repeatmasker/diff_counts.txt, chr6_repeatmasker/occ_counts.txt, chr6_repeatmasker/HD_counts.txt
    jobid: 4
    reason: Missing output files: chr6_repeatmasker/HD_counts.txt, chr6_repeatmasker/occ_counts.txt
    wildcards: name=chr6_repeatmasker
    resources: tmpdir=/tmp

Terminating processes on user request, this might take some time.
[Mon May  5 20:09:28 2025]
Error in rule run_kmer_info:
    jobid: 4
    input: ../sequence_data/chr6.fasta
    output: chr6_repeatmasker/diff_counts.txt, chr6_repeatmasker/occ_counts.txt, chr6_repeatmasker/HD_counts.txt
    shell:
        
        mkdir -p chr6_repeatmasker
        ./kmer_info             -i ../sequence_data/chr6.fasta             -s 559707             -l 100000             -k 20             -o chr6_repeatmasker
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Complete log: .snakemake/log/2025-05-05T200807.736812.snakemake.log
WorkflowError:
At least one job did not complete successfully.
